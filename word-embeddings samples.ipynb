{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import brown, stopwords\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "Fulton\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "Friday\n",
      "an\n",
      "investigation\n",
      "of\n",
      "Atlanta's\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "``\n",
      "no\n",
      "evidence\n",
      "''\n",
      "that\n",
      "any\n",
      "irregularities\n",
      "took\n",
      "place\n",
      ".\n",
      "The\n",
      "jury\n",
      "further\n",
      "said\n",
      "in\n",
      "term-end\n",
      "presentments\n",
      "that\n",
      "the\n",
      "City\n",
      "Executive\n",
      "Committee\n",
      ",\n",
      "which\n",
      "had\n",
      "over-all\n",
      "charge\n",
      "of\n",
      "the\n",
      "election\n",
      ",\n",
      "``\n",
      "deserves\n",
      "the\n",
      "praise\n",
      "and\n",
      "thanks\n",
      "of\n",
      "the\n",
      "City\n",
      "of\n",
      "Atlanta\n",
      "''\n",
      "for\n",
      "the\n",
      "manner\n",
      "in\n",
      "which\n",
      "the\n",
      "election\n",
      "was\n",
      "conducted\n",
      ".\n",
      "The\n",
      "September-October\n",
      "term\n",
      "jury\n",
      "had\n",
      "been\n",
      "charged\n",
      "by\n",
      "Fulton\n",
      "Superior\n",
      "Court\n",
      "Judge\n",
      "Durwood\n",
      "Pye\n",
      "to\n",
      "investigate\n",
      "reports\n",
      "of\n",
      "possible\n",
      "``\n",
      "irregularities\n",
      "''\n",
      "in\n",
      "the\n",
      "hard-fought\n",
      "primary\n",
      "which\n",
      "was\n",
      "won\n",
      "by\n",
      "Mayor-nominate\n",
      "Ivan\n",
      "Allen\n",
      "Jr.\n",
      ".\n",
      "``\n",
      "Only\n",
      "a\n",
      "relative\n",
      "handful\n",
      "of\n",
      "such\n",
      "reports\n",
      "was\n",
      "received\n",
      "''\n",
      ",\n",
      "the\n",
      "jury\n",
      "said\n",
      ",\n",
      "``\n",
      "considering\n",
      "the\n",
      "widespread\n",
      "interest\n",
      "in\n",
      "the\n",
      "election\n",
      ",\n",
      "the\n",
      "number\n",
      "of\n",
      "voters\n",
      "and\n",
      "the\n",
      "size\n",
      "of\n",
      "this\n",
      "city\n",
      "''\n",
      ".\n",
      "The\n",
      "jury\n",
      "said\n",
      "it\n",
      "did\n",
      "find\n",
      "that\n",
      "many\n",
      "of\n",
      "Georgia's\n",
      "registration\n",
      "and\n",
      "election\n",
      "laws\n",
      "``\n",
      "are\n",
      "outmoded\n",
      "or\n",
      "inadequate\n",
      "and\n",
      "often\n",
      "ambiguous\n",
      "''\n",
      ".\n",
      "It\n",
      "recommended\n",
      "that\n",
      "Fulton\n",
      "legislators\n",
      "act\n",
      "``\n",
      "to\n",
      "have\n",
      "these\n",
      "laws\n",
      "studied\n",
      "and\n",
      "revised\n",
      "to\n",
      "the\n",
      "end\n",
      "of\n",
      "modernizing\n",
      "and\n",
      "improving\n",
      "them\n",
      "''\n",
      ".\n",
      "The\n",
      "grand\n",
      "jury\n",
      "commented\n",
      "on\n",
      "a\n",
      "number\n",
      "of\n",
      "other\n",
      "topics\n",
      ",\n",
      "among\n",
      "them\n",
      "the\n",
      "Atlanta\n",
      "and\n",
      "Fulton\n",
      "County\n",
      "purchasing\n",
      "departments\n",
      "which\n",
      "it\n",
      "said\n",
      "``\n",
      "are\n",
      "well\n",
      "operated\n",
      "and\n",
      "follow\n",
      "generally\n",
      "accepted\n",
      "practices\n",
      "which\n",
      "inure\n",
      "to\n",
      "the\n",
      "best\n",
      "interest\n",
      "of\n",
      "both\n",
      "governments\n",
      "''\n",
      ".\n",
      "Merger\n",
      "proposed\n",
      "However\n",
      ",\n",
      "the\n",
      "jury\n",
      "said\n",
      "it\n",
      "believes\n",
      "``\n",
      "these\n",
      "two\n",
      "offices\n",
      "should\n",
      "be\n",
      "combined\n",
      "to\n",
      "achieve\n",
      "greater\n"
     ]
    }
   ],
   "source": [
    "#The corpus consists of 500 samples of text drawn from a wide range of sources. When these are concatenated, they form a very long stream of over a million words, which is available as brown.words(). Let's look at the first 50 words\n",
    "for i in range(250):\n",
    "    print (brown.words()[i],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before doing anything else, let's remove stopwords and punctuation and make everything lowercase. The resulting sequence will be stored in my_word_strea\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "word_stream = [str(w).lower() for w in brown.words() if w.lower() not in my_stopwords]\n",
    "my_word_stream = [w for w in word_stream if (len(w) > 1 and w.isalnum())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fulton',\n",
       " 'county',\n",
       " 'grand',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'friday',\n",
       " 'investigation',\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " 'evidence',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'presentments',\n",
       " 'city',\n",
       " 'executive',\n",
       " 'committee',\n",
       " 'charge',\n",
       " 'election',\n",
       " 'deserves',\n",
       " 'praise',\n",
       " 'thanks',\n",
       " 'city',\n",
       " 'atlanta',\n",
       " 'manner',\n",
       " 'election',\n",
       " 'conducted',\n",
       " 'term',\n",
       " 'jury',\n",
       " 'charged',\n",
       " 'fulton',\n",
       " 'superior',\n",
       " 'court',\n",
       " 'judge',\n",
       " 'durwood',\n",
       " 'pye']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_word_stream[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing co-occurrence probabilities\n",
    "#Step 1: Get a list of words and their frequencies.\n",
    "N = len(my_word_stream)\n",
    "words = []\n",
    "totals = {}\n",
    "for i in range(1, N-1):\n",
    "    w = my_word_stream[i]\n",
    "    if w not in words:\n",
    "        words.append(w)\n",
    "        totals[w] = 0\n",
    "    totals[w] = totals[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Decide on the vocabulary. There are two potentially distinct vocabularies: the words for which we will obtain embeddings (vocab_words) and the words we will consider when looking at context information (context_words). We will take the former to be all words that occur at least 20 times, and the latter to be all words that occur at least 100 times. These choices are pretty arbitrary: by all means, play around with them and find something bette\n",
    "vocab_words = [w for w in words if totals[w] > 19]\n",
    "context_words = [w for w in words if totals[w] > 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4720, 918)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_words), len(context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Get co-occurrence counts. These are defined as follows, for a small constant window_size=2.\n",
    "\n",
    "#Let w0 be any word in vocab_words and w any word in context_words.\n",
    "#Each time w0 occurs in the corpus, look at the window of window_size words before and after it. If w appears in this window, we say it appears in the context of (this particular occurrence of) w0.\n",
    "#Define counts[w0][w] as the total number of times w occurs in the context of w0.\n",
    "#The function get_counts computes the counts array, and returns it as a dictionary (of dictionaries).\n",
    "def get_counts(window_size=2):\n",
    "    counts = {}\n",
    "    for w0 in vocab_words:\n",
    "        counts[w0] = {}\n",
    "    for i in range(window_size, N-window_size):\n",
    "        w0 = my_word_stream[i]\n",
    "        if w0 in vocab_words:\n",
    "            for j in (list(range(-window_size,0)) + list(range(1,window_size+1))):\n",
    "                w = my_word_stream[i+j]\n",
    "                if w in context_words:\n",
    "                    if w not in counts[w0].keys():\n",
    "                        counts[w0][w] = 1\n",
    "                    else:\n",
    "                        counts[w0][w] = counts[w0][w] + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define probs[w0][] to be the distribution over the context of w0, that is:\n",
    "\n",
    "#probs[w0][w] = counts[w0][w] / (sum of all counts[w0][])\n",
    "#This is computed by the function get_co_occurrence_dictionary, given counts.\n",
    "def get_co_occurrence_dictionary(counts):\n",
    "    probs = {}\n",
    "    for w0 in counts.keys():\n",
    "        sum = 0\n",
    "        for w in counts[w0].keys():\n",
    "            sum = sum + counts[w0][w]\n",
    "        if sum > 0:\n",
    "            probs[w0] = {}\n",
    "            for w in counts[w0].keys():\n",
    "                probs[w0][w] = float(counts[w0][w])/float(sum)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final piece of information we need is the frequency of different context words. The function below, get_context_word_distribution, takes counts as input and returns (again, in dictionary form) the array:\n",
    "\n",
    "#context_frequency[w] = sum of all counts[][w] / sum of all counts[][]\n",
    "def get_context_word_distribution(counts):\n",
    "    counts_context = {}\n",
    "    sum_context = 0\n",
    "    context_frequency = {}\n",
    "    for w in context_words:\n",
    "        counts_context[w] = 0\n",
    "    for w0 in counts.keys():\n",
    "        for w in counts[w0].keys():\n",
    "            counts_context[w] = counts_context[w] + counts[w0][w]\n",
    "            sum_context = sum_context + counts[w0][w]\n",
    "    for w in context_words:\n",
    "        context_frequency[w] = float(counts_context[w])/float(sum_context)\n",
    "    return context_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing counts and distributions\n",
      "Computing pointwise mutual information\n"
     ]
    }
   ],
   "source": [
    "#3. The embedding\n",
    "#Based on the various pieces of information above, we compute the pointwise mutual information matrix:\n",
    "\n",
    "#PMI[i,j] = MAX(0, log probs[ith vocab word][jth context word] - log context_frequency[jth context word])\n",
    "#The embedding of any word can then be taken as the corresponding row of this matrix. However, to reduce the dimension, we will apply PCA.\n",
    "print (\"Computing counts and distributions\")\n",
    "counts = get_counts(2)\n",
    "probs = get_co_occurrence_dictionary(counts)\n",
    "context_frequency = get_context_word_distribution(counts)\n",
    "#\n",
    "print (\"Computing pointwise mutual information\")\n",
    "n_vocab = len(vocab_words)\n",
    "n_context = len(context_words)\n",
    "pmi = np.zeros((n_vocab, n_context))\n",
    "for i in range(0, n_vocab):\n",
    "    w0 = vocab_words[i]\n",
    "    for w in probs[w0].keys():\n",
    "        j = context_words.index(w)\n",
    "        pmi[i,j] = max(0.0, np.log(probs[w0][w]) - np.log(context_frequency[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now reduce the dimension of the PMI vectors using principal component analysis. Here we bring it down to 100 dimensions, and then normalize the vectors to unit length\n",
    "pca = PCA(n_components=100)\n",
    "vecs = pca.fit_transform(pmi)\n",
    "for i in range(0,n_vocab):\n",
    "    vecs[i] = vecs[i]/np.linalg.norm(vecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is useful to save this embedding so that it doesn't need to be computed every time.\n",
    "fd = open(\"embedding.pickle\", \"wb\")\n",
    "pickle.dump(vocab_words, fd)\n",
    "pickle.dump(context_words, fd)\n",
    "pickle.dump(vecs, fd)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can get some insight into the embedding by looking at the nearest neighbor of different words in the embedded space.\n",
    "def word_NN(w):\n",
    "    if not(w in vocab_words):\n",
    "        print (\"Unknown word\")\n",
    "        return\n",
    "    v = vecs[vocab_words.index(w)]\n",
    "    neighbor = 0\n",
    "    curr_dist = np.linalg.norm(v - vecs[0])\n",
    "    for i in range(1, n_vocab):\n",
    "        dist = np.linalg.norm(v - vecs[i])\n",
    "        if (dist < curr_dist) and (dist > 0.0):\n",
    "            neighbor = i\n",
    "            curr_dist = dist\n",
    "    return vocab_words[neighbor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artery'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('pulmonary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'era'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('communism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown word\n"
     ]
    }
   ],
   "source": [
    "word_NN('ass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sunday'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('friday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown word\n"
     ]
    }
   ],
   "source": [
    "word_NN('be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'later'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('london')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
